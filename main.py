#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import cv2
import argparse
import numpy as np
from tqdm import tqdm
import time
import sys

# Th√™m th∆∞ m·ª•c g·ªëc v√†o path ƒë·ªÉ c√≥ th·ªÉ import c√°c module
sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

# Import c√°c module t·ª± vi·∫øt
from detectors.yolo_detector import YOLODetector  # S·ª≠ d·ª•ng YOLODetector m·ªõi thay th·∫ø cho YOLOv5Detector c≈©
from trackers.deep_sort.deep_sort import DeepSORT
from core_utils.drawing import visualize_detections, visualize_tracks
from core_utils.file_io import create_output_dirs, create_tracking_log, log_tracking_results
from core_utils.counter import PersonCounter

def parse_args():
    """X·ª≠ l√Ω c√°c tham s·ªë d√≤ng l·ªánh"""
    parser = argparse.ArgumentParser(description='YOLOv5 + DeepSORT Tracking Demo')
    
    # Tham s·ªë ƒë·∫ßu v√†o
    parser.add_argument('--source', type=str, default='data/MOT16/train/MOT16-02',
                        help='Th∆∞ m·ª•c chuy·ªÉn ƒë·ªông MOT16 ho·∫∑c ƒë∆∞·ªùng d·∫´n video/camera')
    parser.add_argument('--output-dir', type=str, default='outputs',
                        help='Th∆∞ m·ª•c ƒë·∫ßu ra')
    
    # Tham s·ªë ƒë·∫øm ng∆∞·ªùi
    parser.add_argument('--counter-direction', type=str, default='horizontal',
                        choices=['horizontal', 'vertical'],
                        help='H∆∞·ªõng c·ªßa ƒë∆∞·ªùng ƒë·∫øm (horizontal/vertical)')
    parser.add_argument('--counter-line', type=float, default=0.5,
                        help='V·ªã tr√≠ c·ªßa ƒë∆∞·ªùng ƒë·∫øm (0.0-1.0, t·ª∑ l·ªá ph·∫ßn trƒÉm c·ªßa chi·ªÅu cao/r·ªông)')
    
    # Tham s·ªë YOLO
    parser.add_argument('--yolo-model', type=str, default='yolov5s',
                        help='T√™n m√¥ h√¨nh YOLOv5 (yolov5s, yolov5m, v.v.)')
    parser.add_argument('--conf-thres', type=float, default=0.25,
                        help='Ng∆∞·ª°ng tin c·∫≠y YOLO')
    parser.add_argument('--classes', nargs='+', type=int, default=[0],
                        help='C√°c class c·∫ßn l·ªçc (0: ng∆∞·ªùi)')
    parser.add_argument('--use-gt', action='store_true',
                        help='S·ª≠ d·ª•ng d·ªØ li·ªáu ground truth n·∫øu c√≥')
    parser.add_argument('--use-yolo', action='store_true',
                        help='∆Øu ti√™n s·ª≠ d·ª•ng YOLO ngay c·∫£ khi c√≥ ground truth')
    parser.add_argument('--detection-method', type=str, default='auto',
                        choices=['auto', 'yolo', 'ground-truth', 'both'],
                        help='Ph∆∞∆°ng ph√°p ph√°t hi·ªán: auto (t·ª± ƒë·ªông), yolo (ch·ªâ d√πng YOLO), ground-truth (ch·ªâ d√πng GT), both (k·∫øt h·ª£p)')
    
    # Tham s·ªë DeepSORT
    parser.add_argument('--deepsort-config', type=str, default='configs/deepsort.yaml',
                        help='File c·∫•u h√¨nh DeepSORT')
    
    # Tham s·ªë hi·ªÉn th·ªã
    parser.add_argument('--display', action='store_true', help='Hi·ªÉn th·ªã khung h√¨nh k·∫øt qu·∫£')
    parser.add_argument('--save-vid', action='store_true', help='L∆∞u video k·∫øt qu·∫£')
    
    args = parser.parse_args()
    return args

def process_mot_sequence(seq_path, args):
    """X·ª≠ l√Ω sequence MOT16"""
    # ƒê∆∞·ªùng d·∫´n ·∫£nh v√† c·∫•u h√¨nh
    img_dir = os.path.join(seq_path, "img1")
    seq_name = os.path.basename(seq_path)
    
    # T·∫°o th∆∞ m·ª•c ƒë·∫ßu ra
    video_output_dir, log_output_dir = create_output_dirs(args.output_dir)
    
    # T·∫°o file log tracking
    log_path = create_tracking_log(log_output_dir, seq_name)
    
    # Kh·ªüi t·∫°o b·ªô ƒë·∫øm ng∆∞·ªùi
    counter_position = args.counter_line
    counter_direction = args.counter_direction
    
    # Danh s√°ch file ·∫£nh
    img_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.jpg')])
    
    # L·∫•y k√≠ch th∆∞·ªõc frame
    first_img = cv2.imread(os.path.join(img_dir, img_files[0]))
    frame_height, frame_width = first_img.shape[:2]
    
    # Kh·ªüi t·∫°o b·ªô ƒë·∫øm ng∆∞·ªùi v·ªõi k√≠ch th∆∞·ªõc frame
    person_counter = PersonCounter(frame_height, frame_width, 
                               line_position=counter_position,
                               line_direction=counter_direction)
    
    # Kh·ªüi t·∫°o video writer n·∫øu c·∫ßn
    video_writer = None
    if args.save_vid:
        video_path = os.path.join(video_output_dir, f"{seq_name}_tracking.mp4")
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        video_writer = cv2.VideoWriter(video_path, fourcc, 30, (frame_width, frame_height))
        print(f"üé• Video output: {video_path}")
    
    # B·∫Øt ƒë·∫ßu tracking
    frame_id = 0  # Frame ID trong m√£ ngu·ªìn
    mot_frame_id = 1  # Frame ID d√πng trong MOT16, b·∫Øt ƒë·∫ßu t·ª´ 1
    total_frames = len(img_files)
    processing_times = []
    
    for img_file in tqdm(img_files, desc=f"Processing {seq_name}"):
        start_time = time.time()
        frame_id += 1
        mot_frame_id = frame_id  # MOT16 frame_id b·∫Øt ƒë·∫ßu t·ª´ 1
        
        # ƒê·ªçc frame
        img_path = os.path.join(img_dir, img_file)
        frame = cv2.imread(img_path)
        
        if frame is None:
            print(f"Warning: Cannot read frame: {img_path}")
            continue
            
        # 1. Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng b·∫±ng YOLO
        # Truy·ªÅn frame_id theo ƒë·ªãnh d·∫°ng MOT16
        detections = detector.detect(frame, frame_id=mot_frame_id)
        
        # Debug: In th√¥ng tin v·ªÅ detections
        if frame_id % 20 == 0:
            print(f"DEBUG: Frame {frame_id} (MOT frame_id={mot_frame_id}) c√≥ {len(detections)} detections")
        
        # 2. C·∫≠p nh·∫≠t tracker
        tracks = tracker.update(frame, detections)
        
        # Debug: In th√¥ng tin v·ªÅ tracks
        if frame_id % 20 == 0 and tracks:
            print(f"DEBUG: Frame {frame_id} c√≥ {len(tracks)} tracks")
        
        # 3. C·∫≠p nh·∫≠t b·ªô ƒë·∫øm ng∆∞·ªùi
        person_counter.update(tracks)
        
        # 4. Ghi log v√† hi·ªÉn th·ªã
        log_tracking_results(log_path, frame_id, tracks)
        
        # 5. V·∫Ω k·∫øt qu·∫£
        if args.display or args.save_vid:
            # V·∫Ω c√°c v·ªã tr√≠ ph√°t hi·ªán (m√†u xanh l√° nh·∫°t)
            vis_img = visualize_detections(frame, detections, args.conf_thres)
            # V·∫Ω c√°c ƒë·ªëi t∆∞·ª£ng ƒëang track (ID + bbox)
            vis_img = visualize_tracks(vis_img, tracks)
            # V·∫Ω ƒë∆∞·ªùng ƒë·∫øm v√† s·ªë li·ªáu th·ªëng k√™
            vis_img = person_counter.draw_counter(vis_img)
            # V·∫Ω th√¥ng tin ph·ª•
            cv2.putText(vis_img, f"Frame: {frame_id}/{total_frames}", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
            
            if args.display:
                cv2.imshow(f"YOLOv5 + DeepSORT - {seq_name}", vis_img)
                if cv2.waitKey(1) == 27:  # Esc key
                    break
            
            if args.save_vid and video_writer:
                video_writer.write(vis_img)
        
        # T√≠nh th·ªùi gian x·ª≠ l√Ω
        processing_time = time.time() - start_time
        processing_times.append(processing_time)
    
    # K·∫øt th√∫c
    if video_writer:
        video_writer.release()
    
    if args.display:
        cv2.destroyAllWindows()
    
    # Th·ªëng k√™
    avg_time = sum(processing_times) / len(processing_times)
    avg_fps = 1.0 / avg_time if avg_time > 0 else 0
    print(f"\n=== Th·ªëng k√™ ===\n")
    print(f"T·ªïng s·ªë frame: {total_frames}")
    print(f"Th·ªùi gian trung b√¨nh m·ªói frame: {avg_time:.4f}s")
    print(f"FPS trung b√¨nh: {avg_fps:.2f}")
    print(f"K·∫øt qu·∫£ tracking ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {log_path}")
    
    # Th·ªëng k√™ ƒë·∫øm ng∆∞·ªùi
    print(f"\n=== Th·ªëng k√™ ƒë·∫øm ng∆∞·ªùi ===\n")
    direction_text = "l√™n/xu·ªëng" if counter_direction == "horizontal" else "tr√°i/ph·∫£i"
    print(f"T·ªïng s·ªë ng∆∞·ªùi ƒëi qua ƒë∆∞·ªùng ƒë·∫øm: {person_counter.total_count}")
    print(f"S·ªë ng∆∞·ªùi ƒëi {direction_text.split('/')[0]}: {person_counter.count_up}")
    print(f"S·ªë ng∆∞·ªùi ƒëi {direction_text.split('/')[1]}: {person_counter.count_down}")

def process_video(video_path, args):
    """X·ª≠ l√Ω video th√¥ng th∆∞·ªùng"""
    # T·∫°o th∆∞ m·ª•c ƒë·∫ßu ra
    video_output_dir, log_output_dir = create_output_dirs(args.output_dir)
    
    # L·∫•y t√™n video
    video_name = os.path.splitext(os.path.basename(video_path))[0]
    
    # T·∫°o file log tracking
    log_path = create_tracking_log(log_output_dir, video_name)
    
    # Kh·ªüi t·∫°o tham s·ªë ƒë·∫øm ng∆∞·ªùi
    counter_position = args.counter_line
    counter_direction = args.counter_direction
    
    # M·ªü video
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Error: Kh√¥ng th·ªÉ m·ªü video {video_path}")
        return
    
    # L·∫•y th√¥ng tin video
    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    # Kh·ªüi t·∫°o b·ªô ƒë·∫øm ng∆∞·ªùi
    person_counter = PersonCounter(frame_height, frame_width, 
                               line_position=counter_position,
                               line_direction=counter_direction)
    
    # Kh·ªüi t·∫°o video writer n·∫øu c·∫ßn
    video_writer = None
    if args.save_vid:
        video_path = os.path.join(video_output_dir, f"{video_name}_tracking.mp4")
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        video_writer = cv2.VideoWriter(video_path, fourcc, fps, (frame_width, frame_height))
        print(f"üé• Video output: {video_path}")
    
    # B·∫Øt ƒë·∫ßu tracking
    frame_id = 0
    processing_times = []
    
    with tqdm(total=total_frames, desc=f"Processing {video_name}") as pbar:
        while cap.isOpened():
            start_time = time.time()
            ret, frame = cap.read()
            
            if not ret:
                break
                
            frame_id += 1
            
            # 1. Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng b·∫±ng YOLO
            detections = detector.detect(frame)
            
            # 2. C·∫≠p nh·∫≠t tracker
            tracks = tracker.update(frame, detections)
            
            # 3. Ghi log v√† hi·ªÉn th·ªã
            log_tracking_results(log_path, frame_id, tracks)
            
            # 3.1 C·∫≠p nh·∫≠t b·ªô ƒë·∫øm ng∆∞·ªùi
            person_counter.update(tracks)
            
            # 4. V·∫Ω k·∫øt qu·∫£
            if args.display or args.save_vid:
                # V·∫Ω c√°c v·ªã tr√≠ ph√°t hi·ªán (m√†u xanh l√° nh·∫°t)
                vis_img = visualize_detections(frame, detections, args.conf_thres)
                # V·∫Ω c√°c ƒë·ªëi t∆∞·ª£ng ƒëang track (ID + bbox)
                vis_img = visualize_tracks(vis_img, tracks)
                # V·∫Ω ƒë∆∞·ªùng ƒë·∫øm v√† th√¥ng tin ƒë·∫øm ng∆∞·ªùi
                vis_img = person_counter.draw_counter(vis_img)
                # V·∫Ω th√¥ng tin ph·ª•
                cv2.putText(vis_img, f"Frame: {frame_id}/{total_frames}", (200, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
                
                if args.display:
                    cv2.imshow(f"YOLOv5 + DeepSORT - {video_name}", vis_img)
                    if cv2.waitKey(1) == 27:  # Esc key
                        break
                
                if args.save_vid and video_writer:
                    video_writer.write(vis_img)
            
            # T√≠nh th·ªùi gian x·ª≠ l√Ω
            processing_time = time.time() - start_time
            processing_times.append(processing_time)
            
            # C·∫≠p nh·∫≠t thanh ti·∫øn tr√¨nh
            pbar.update(1)
    
    # K·∫øt th√∫c
    cap.release()
    if video_writer:
        video_writer.release()
    
    if args.display:
        cv2.destroyAllWindows()
    
    # Th·ªëng k√™
    avg_time = sum(processing_times) / len(processing_times) if processing_times else 0
    avg_fps = 1.0 / avg_time if avg_time > 0 else 0
    print(f"\n=== Th·ªëng k√™ ===\n")
    print(f"T·ªïng s·ªë frame: {frame_id}")
    print(f"Th·ªùi gian trung b√¨nh m·ªói frame: {avg_time:.4f}s")
    print(f"FPS trung b√¨nh: {avg_fps:.2f}")
    print(f"K·∫øt qu·∫£ tracking ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {log_path}")
    
    # Th·ªëng k√™ ƒë·∫øm ng∆∞·ªùi
    print(f"\n=== Th·ªëng k√™ ƒë·∫øm ng∆∞·ªùi ===\n")
    direction_text = "l√™n/xu·ªëng" if counter_direction == "horizontal" else "tr√°i/ph·∫£i"
    print(f"T·ªïng s·ªë ng∆∞·ªùi ƒëi qua ƒë∆∞·ªùng ƒë·∫øm: {person_counter.total_count}")
    print(f"S·ªë ng∆∞·ªùi ƒëi {direction_text.split('/')[0]}: {person_counter.count_up}")
    print(f"S·ªë ng∆∞·ªùi ƒëi {direction_text.split('/')[1]}: {person_counter.count_down}")


if __name__ == "__main__":
    # X·ª≠ l√Ω tham s·ªë d√≤ng l·ªánh
    args = parse_args()
    
    # Kh·ªüi t·∫°o detector
    # X√°c ƒë·ªãnh ph∆∞∆°ng ph√°p ph√°t hi·ªán theo c√°c tham s·ªë d√≤ng l·ªánh
    gt_available = os.path.isdir(args.source) and os.path.exists(os.path.join(args.source, "gt"))
    
    # X·ª≠ l√Ω tham s·ªë detection-method
    if args.detection_method == 'auto':
        # T·ª± ƒë·ªông ch·ªçn ph∆∞∆°ng ph√°p t·ªët nh·∫•t:
        # - D√πng ground truth n·∫øu c√≥ v√† --use-yolo kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        # - D√πng YOLO trong c√°c tr∆∞·ªùng h·ª£p kh√°c
        use_gt = gt_available and not args.use_yolo
    elif args.detection_method == 'yolo':
        # Ch·ªâ d√πng YOLO
        use_gt = False
    elif args.detection_method == 'ground-truth':
        # Ch·ªâ d√πng ground truth n·∫øu c√≥
        use_gt = gt_available
        if not gt_available:
            print(f"\n‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ground truth trong {args.source}/gt")
            print("S·∫Ω s·ª≠ d·ª•ng YOLOv5 thay th·∫ø.")
    elif args.detection_method == 'both':
        # S·ª≠ d·ª•ng c·∫£ hai (hi·ªán t·∫°i ch∆∞a h·ªó tr·ª£ ƒë·∫ßy ƒë·ªß, s·∫Ω s·ª≠ d·ª•ng GT n·∫øu c√≥)
        use_gt = gt_available
        print("\n‚ÑπÔ∏è Ch·∫ø ƒë·ªô 'both' hi·ªán t·∫°i s·∫Ω ∆∞u ti√™n d√πng ground truth n·∫øu c√≥, ng∆∞·ª£c l·∫°i s·∫Ω d√πng YOLO.")
        
    # Tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát: N·∫øu ch·ªâ ƒë·ªãnh --use-gt, ∆∞u ti√™n d√πng GT n·∫øu c√≥
    if args.use_gt and gt_available:
        use_gt = True
        
    # Tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát: N·∫øu ch·ªâ ƒë·ªãnh --use-yolo, ∆∞u ti√™n d√πng YOLO
    if args.use_yolo:
        use_gt = False
        
    # Hi·ªÉn th·ªã ph∆∞∆°ng ph√°p ƒë∆∞·ª£c ch·ªçn
    if use_gt:
        print(f"\n‚ÑπÔ∏è S·ª≠ d·ª•ng d·ªØ li·ªáu GROUND TRUTH t·ª´ {args.source}/gt")
    else:
        print(f"\n‚ÑπÔ∏è S·ª≠ d·ª•ng m√¥ h√¨nh YOLOv5 {args.yolo_model}")

    
    print("\n=== Kh·ªüi t·∫°o YOLO Detector ===\n")
    # Kh·ªüi t·∫°o YOLODetector m·ªõi v·ªõi ƒë·∫ßy ƒë·ªß tham s·ªë
    detector = YOLODetector(
        model_name=args.yolo_model,
        conf_thres=args.conf_thres,
        classes=args.classes,
        use_gt=use_gt
    )
    
    # N·∫øu ngu·ªìn d·ªØ li·ªáu l√† MOT16, th·ª≠ t·∫£i ground truth
    if use_gt:
        detector.load_gt_if_available(args.source)
    
    # Kh·ªüi t·∫°o tracker
    print("\n=== Kh·ªüi t·∫°o DeepSORT Tracker ===\n")
    tracker = DeepSORT(config_path=args.deepsort_config)
    
    # Ki·ªÉm tra lo·∫°i ƒë·∫ßu v√†o
    source = args.source
    
    if os.path.isdir(source) and os.path.exists(os.path.join(source, "img1")):
        # X·ª≠ l√Ω sequence MOT16
        print(f"\n=== X·ª≠ l√Ω sequence MOT16: {source} ===\n")
        process_mot_sequence(source, args)
    elif os.path.isfile(source) and source.endswith((".mp4", ".avi", ".mov")):
        # X·ª≠ l√Ω video th√¥ng th∆∞·ªùng
        print(f"\n=== X·ª≠ l√Ω video: {source} ===\n")
        process_video(source, args)
    elif source.isdigit():
        # X·ª≠ l√Ω webcam
        print(f"\n=== B·∫Øt ƒë·∫ßu webcam ID: {source} ===\n") 
        process_video(int(source), args)
    else:
        print(f"Error: Kh√¥ng h·ªó tr·ª£ lo·∫°i ƒë·∫ßu v√†o: {source}")
